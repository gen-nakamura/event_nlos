{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch       : 2.1.2\n",
      "torchvision : 0.16.2\n",
      "device      : cuda\n",
      "12.1\n",
      "1\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# バージョン確認\n",
    "print(\"torch       :\", torch.__version__)\n",
    "print(\"torchvision :\", torchvision.__version__)\n",
    "\n",
    "# GPUの確認\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device      :\", device)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.device_count())\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional\n",
    "# import cv2\n",
    "# from PIL import Image\n",
    "# from matplotlib import pyplot as plt\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.utils.data import Dataset as BaseDataset\n",
    "# # !pip install -U segmentation-models-pytorch\n",
    "# import segmentation_unets_pytorch as smp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, middle_channels, kernel_size = 5, padding=\"same\")\n",
    "        self.bn1 = nn.BatchNorm2d(middle_channels)\n",
    "        self.rl = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(middle_channels, out_channels, kernel_size = 5, padding=\"same\")\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('forward TCB')\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.rl(x)\n",
    "        # print('first TCB, x: ', x.shape)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.rl(x)\n",
    "        # print('second TCB, x: ', x.shape)\n",
    "        return x\n",
    "\n",
    "class UNet_2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.TCB1 = TwoConvBlock(150, 150, 150)\n",
    "        self.TCB2 = TwoConvBlock(150, 150, 150)\n",
    "        self.maxpool = nn.MaxPool2d(2, stride = 2)\n",
    "        self.flatten = nn.Flatten(2, -1)\n",
    "        self.linear = nn.Linear(1200, 2)\n",
    "        \n",
    "        # self.conv1 = nn.Conv2d(64, 4, kernel_size = 1)\n",
    "        # self.soft = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('forward')\n",
    "        # print('x0: ', x)\n",
    "        x = self.TCB1(x)\n",
    "        # x1 = x\n",
    "        x = self.maxpool(x)\n",
    "        # print('x1: ', x1.shape)\n",
    "\n",
    "        x = self.TCB2(x)\n",
    "        # x2 = x\n",
    "        x = self.maxpool(x)\n",
    "        # print('x2: ', x2.shape)\n",
    "\n",
    "        x = self.TCB2(x)\n",
    "        # x3 = x\n",
    "        x = self.maxpool(x)\n",
    "        # print('x3: ', x3.shape)\n",
    "\n",
    "        x = self.TCB2(x)\n",
    "        # x4 = x\n",
    "        x = self.maxpool(x)\n",
    "        # print('x4: ', x4.shape)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        # x5 = x\n",
    "        # print('x5: ', x5.shape)\n",
    "\n",
    "        x = self.linear(x)\n",
    "        # x6 = x\n",
    "        # print('x6: ', x6.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"earlystoppingクラス\"\"\"\n",
    "\n",
    "    def __init__(self, patience=5, verbose=False, path='checkpoint_model.pth'):\n",
    "        \"\"\"引数：最小値の非更新数カウンタ、表示設定、モデル格納path\"\"\"\n",
    "\n",
    "        self.patience = patience    #設定ストップカウンタ\n",
    "        self.verbose = verbose      #表示の有無\n",
    "        self.counter = 0            #現在のカウンタ値\n",
    "        self.best_score = None      #ベストスコア\n",
    "        self.early_stop = False     #ストップフラグ\n",
    "        self.val_loss_min = np.Inf   #前回のベストスコア記憶用\n",
    "        self.path = path             #ベストモデル格納path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        特殊(call)メソッド\n",
    "        実際に学習ループ内で最小lossを更新したか否かを計算させる部分\n",
    "        \"\"\"\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:  #1Epoch目の処理\n",
    "            self.best_score = score   #1Epoch目はそのままベストスコアとして記録する\n",
    "            self.checkpoint(val_loss, model)  #記録後にモデルを保存してスコア表示する\n",
    "        elif score < self.best_score:  # ベストスコアを更新できなかった場合\n",
    "            self.counter += 1   #ストップカウンタを+1\n",
    "            if self.verbose:  #表示を有効にした場合は経過を表示\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')  #現在のカウンタを表示する \n",
    "            if self.counter >= self.patience:  #設定カウントを上回ったらストップフラグをTrueに変更\n",
    "                self.early_stop = True\n",
    "        else:  #ベストスコアを更新した場合\n",
    "            self.best_score = score  #ベストスコアを上書き\n",
    "            self.checkpoint(val_loss, model)  #モデルを保存してスコア表示\n",
    "            self.counter = 0  #ストップカウンタリセット\n",
    "\n",
    "    def checkpoint(self, val_loss, model):\n",
    "        '''ベストスコア更新時に実行されるチェックポイント関数'''\n",
    "        if self.verbose:  #表示を有効にした場合は、前回のベストスコアからどれだけ更新したか？を表示\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)  #ベストモデルを指定したpathに保存\n",
    "        self.val_loss_min = val_loss  #その時のlossを記録する\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgen0401\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gen0401/Documents/research/codes/event_nlos/wandb/run-20240109_143135-xxc6ey0z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gen0401/event-based_NLOS/runs/xxc6ey0z' target=\"_blank\">aug_bin150_batch6_normdata950_lr0001_ReLU_MSE_epoch1_NormBN</a></strong> to <a href='https://wandb.ai/gen0401/event-based_NLOS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gen0401/event-based_NLOS' target=\"_blank\">https://wandb.ai/gen0401/event-based_NLOS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gen0401/event-based_NLOS/runs/xxc6ey0z' target=\"_blank\">https://wandb.ai/gen0401/event-based_NLOS/runs/xxc6ey0z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1]\n",
      "Data No.00416 was loadedbatch:  0\n",
      "Data No.00582 was loadedbatch:  1\n",
      "Data No.00065 was loadedbatch:  2\n",
      "Data No.00835 was loadedbatch:  3\n",
      "Data No.00585 was loadedbatch:  4\n",
      "Data No.00553 was loadedbatch:  5\n",
      "Data No.00210 was loadedbatch:  6\n",
      "Data No.00736 was loadedbatch:  7\n",
      "Data No.00844 was loadedbatch:  8\n",
      "Data No.00984 was loadedbatch:  9\n",
      "Data No.00215 was loadedbatch:  10\n",
      "Data No.00315 was loadedbatch:  11\n",
      "Data No.00486 was loadedbatch:  12\n",
      "Data No.00143 was loadedbatch:  13\n",
      "Data No.00766 was loadedbatch:  14\n",
      "Data No.00012 was loadedbatch:  15\n",
      "Data No.00352 was loadedbatch:  16\n",
      "Data No.00073 was loadedbatch:  17\n",
      "Data No.00058 was loadedbatch:  18\n",
      "Data No.00590 was loadedbatch:  19\n",
      "Data No.00427 was loadedbatch:  20\n",
      "Data No.00111 was loadedbatch:  21\n",
      "Data No.00852 was loadedbatch:  22\n",
      "Data No.00229 was loadedbatch:  23\n",
      "Data No.00336 was loadedbatch:  24\n",
      "Data No.00285 was loadedbatch:  25\n",
      "Data No.00329 was loadedbatch:  26\n",
      "Data No.00888 was loadedbatch:  27\n",
      "Data No.00440 was loadedbatch:  28\n",
      "Data No.00708 was loadedbatch:  29\n",
      "Data No.00653 was loadedbatch:  30\n",
      "Data No.00338 was loadedbatch:  31\n",
      "Data No.00294 was loadedbatch:  32\n",
      "Data No.00108 was loadedbatch:  33\n",
      "Data No.00648 was loadedbatch:  34\n",
      "Data No.00228 was loadedbatch:  35\n",
      "Data No.00895 was loadedbatch:  36\n",
      "Data No.00973 was loadedbatch:  37\n",
      "Data No.00450 was loadedbatch:  38\n",
      "Data No.00296 was loadedbatch:  39\n",
      "Data No.00407 was loadedbatch:  40\n",
      "Data No.00410 was loadedbatch:  41\n",
      "Data No.00960 was loadedbatch:  42\n",
      "Data No.00838 was loadedbatch:  43\n",
      "Data No.00901 was loadedbatch:  44\n",
      "Data No.00977 was loadedbatch:  45\n",
      "Data No.00084 was loadedbatch:  46\n",
      "Data No.00461 was loadedbatch:  47\n",
      "Data No.00378 was loadedbatch:  48\n",
      "Data No.00535 was loadedbatch:  49\n",
      "Data No.00538 was loadedbatch:  50\n",
      "Data No.00603 was loadedbatch:  51\n",
      "Data No.00896 was loadedbatch:  52\n",
      "Data No.00147 was loadedbatch:  53\n",
      "Data No.00211 was loadedbatch:  54\n",
      "Data No.00772 was loadedbatch:  55\n",
      "Data No.00323 was loadedbatch:  56\n",
      "Data No.00300 was loadedbatch:  57\n",
      "Data No.00657 was loadedbatch:  58\n",
      "Data No.00862 was loadedbatch:  59\n",
      "Data No.00376 was loadedbatch:  60\n",
      "Data No.00804 was loadedbatch:  61\n",
      "Data No.00566 was loadedbatch:  62\n",
      "Data No.00696 was loadedbatch:  63\n",
      "Data No.00863 was loadedbatch:  64\n",
      "Data No.00874 was loadedbatch:  65\n",
      "Data No.00403 was loadedbatch:  66\n",
      "Data No.00507 was loadedbatch:  67\n",
      "Data No.00948 was loadedbatch:  68\n",
      "Data No.00775 was loadedbatch:  69\n",
      "Data No.00074 was loadedbatch:  70\n",
      "Data No.00695 was loadedbatch:  71\n",
      "Data No.00386 was loadedbatch:  72\n",
      "Data No.00537 was loadedbatch:  73\n",
      "Data No.00357 was loadedbatch:  74\n",
      "Data No.00156 was loadedbatch:  75\n",
      "Data No.00909 was loadedbatch:  76\n",
      "Data No.00580 was loadedbatch:  77\n",
      "Data No.00575 was loadedbatch:  78\n",
      "Data No.00944 was loadedbatch:  79\n",
      "Data No.00270 was loadedbatch:  80\n",
      "Data No.00621 was loadedbatch:  81\n",
      "Data No.00353 was loadedbatch:  82\n",
      "Data No.00765 was loadedbatch:  83\n",
      "Data No.00292 was loadedbatch:  84\n",
      "Data No.00425 was loadedbatch:  85\n",
      "Data No.00998 was loadedbatch:  86\n",
      "Data No.00112 was loadedbatch:  87\n",
      "Data No.00243 was loadedbatch:  88\n",
      "Data No.00496 was loadedbatch:  89\n",
      "Data No.00158 was loadedbatch:  90\n",
      "Data No.00719 was loadedbatch:  91\n",
      "Data No.00359 was loadedbatch:  92\n",
      "Data No.00986 was loadedbatch:  93\n",
      "Data No.00138 was loadedbatch:  94\n",
      "Data No.00052 was loadedbatch:  95\n",
      "Data No.00577 was loadedbatch:  96\n",
      "Data No.00840 was loadedbatch:  97\n",
      "Data No.00579 was loadedbatch:  98\n",
      "Data No.00254 was loadedbatch:  99\n",
      "Data No.00448 was loadedbatch:  100\n",
      "Data No.00920 was loadedbatch:  101\n",
      "Data No.00406 was loadedbatch:  102\n",
      "Data No.00744 was loadedbatch:  103\n",
      "Data No.00593 was loadedbatch:  104\n",
      "Data No.00632 was loadedbatch:  105\n",
      "Data No.00289 was loadedbatch:  106\n",
      "Data No.00169 was loadedbatch:  107\n",
      "Data No.00820 was loadedbatch:  108\n",
      "Data No.00655 was loadedbatch:  109\n",
      "Data No.00365 was loadedbatch:  110\n",
      "Data No.00882 was loadedbatch:  111\n",
      "Data No.00432 was loadedbatch:  112\n",
      "Data No.00899 was loadedbatch:  113\n",
      "Data No.00784 was loadedbatch:  114\n",
      "Data No.00592 was loadedbatch:  115\n",
      "Data No.00690 was loadedbatch:  116\n",
      "Data No.00053 was loadedbatch:  117\n",
      "Data No.00905 was loadedbatch:  118\n",
      "Data No.00119 was loadedbatch:  119\n",
      "Data No.00659 was loadedbatch:  120\n",
      "Data No.00966 was loadedbatch:  121\n",
      "Data No.00890 was loadedbatch:  122\n",
      "Data No.00348 was loadedbatch:  123\n",
      "Data No.00122 was loadedbatch:  124\n",
      "Data No.00764 was loadedbatch:  125\n",
      "Data No.00153 was loadedbatch:  126\n",
      "Data No.00314 was loadedbatch:  127\n",
      "Data No.00869 was loadedbatch:  128\n",
      "Data No.00788 was loadedbatch:  129\n",
      "Data No.00113 was loadedbatch:  130\n",
      "Data No.00005 was loadedbatch:  131\n",
      "Data No.00880 was loadedbatch:  132\n",
      "Data No.00608 was loadedbatch:  133\n",
      "Data No.00094 was loadedbatch:  134\n",
      "Data No.00110 was loadedbatch:  135\n",
      "Data No.00981 was loadedbatch:  136\n",
      "Data No.00419 was loadedbatch:  137\n",
      "Data No.00980 was loadedbatch:  138\n",
      "Data No.00468 was loadedbatch:  139\n",
      "Data No.00610 was loadedbatch:  140\n",
      "Data No.00320 was loadedbatch:  141\n",
      "Data No.00733 was loadedbatch:  142\n",
      "Data No.00393 was loadedbatch:  143\n",
      "Data No.00226 was loadedbatch:  144\n",
      "Data No.00325 was loadedbatch:  145\n",
      "Data No.00397 was loadedbatch:  146\n",
      "Data No.00411 was loadedbatch:  147\n",
      "Data No.00763 was loadedbatch:  148\n",
      "Data No.00739 was loadedbatch:  149\n",
      "Data No.00814 was loadedbatch:  150\n",
      "Data No.00806 was loadedbatch:  151\n",
      "Data No.00075 was loadedbatch:  152\n",
      "Data No.00367 was loadedbatch:  153\n",
      "Data No.00501 was loadedbatch:  154\n",
      "Data No.00056 was loadedbatch:  155\n",
      "Data No.00514 was loadedbatch:  156\n",
      "Data No.00878 was loadedbatch:  157\n",
      "Data No.00530 was loadedbatch:  158\n",
      "last batch value:  158\n",
      "loss_num:  744881.625\n",
      "Data No.00132 was loaded\n",
      "-------------------------------\n",
      "Done!\n",
      "Saved PyTorch Model: ./Dataset/pretrained/model_aug_bin150_batch6_normdata950_lr0001_ReLU_MSE_epoch1_NormBN.pth\n",
      "Start: 20240109-143137\n",
      "End: 20240109-143900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_loss</td><td>829387.6875</td></tr><tr><td>train_loss</td><td>744881.625</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">aug_bin150_batch6_normdata950_lr0001_ReLU_MSE_epoch1_NormBN</strong> at: <a href='https://wandb.ai/gen0401/event-based_NLOS/runs/xxc6ey0z' target=\"_blank\">https://wandb.ai/gen0401/event-based_NLOS/runs/xxc6ey0z</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240109_143135-xxc6ey0z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import wandb\n",
    "# from model import E2VIDRecurrent, E2VID\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# 変更する必要のある変数\n",
    "TITLE = \"BrandSilence\"\n",
    "num_bin = 150\n",
    "\n",
    "input_path = './Dataset/train_data/'\n",
    "target_path = './Dataset/validation_data/'\n",
    "SIZE_X = 640\n",
    "SIZE_Y = 480\n",
    "learning_rate = 0.0001\n",
    "epochs = 8\n",
    "Loss = \"MSE\"\n",
    "activation = \"ReLU\"\n",
    "batch_size=6\n",
    "norm = \"BN\"\n",
    "flip_lr = 0.4\n",
    "flip_ud = 0.4\n",
    "\n",
    "now = datetime.now()\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.title(\"Loss\")\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "\n",
    "# 損失関数の記述\n",
    "# TverskyLoss = smp.losses.TverskyLoss(mode='multilabel', log_loss=False)\n",
    "# BCELoss     = smp.losses.SoftBCEWithLogitsLoss()\n",
    "# def criterion(pred,target):\n",
    "#     return 0.5*BCELoss(pred, target) + 0.5*TverskyLoss(pred, target)\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, input_dir, target_dir, title):\n",
    "        self.dir_input = input_dir\n",
    "        self.dir_target = target_dir\n",
    "        self.title = title\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(glob.glob(self.dir_target + f\"*_{num_bin}.npz\"))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #rd_lr = random.random()\n",
    "        #rd_ud = random.random()\n",
    "        input_path = self.dir_input + self.title + str(idx).zfill(5) + f\"_{num_bin}.npz\"\n",
    "        input_loaded = np.load(input_path)\n",
    "        input = input_loaded['arr_0']\n",
    "        target_path = self.dir_target + self.title + str(idx).zfill(5) + f\"_{num_bin}.npz\"\n",
    "        target = np.load(target_path)['arr_0']\n",
    "        print(\"\\rData No.\" + str(idx).zfill(5) + \" was loaded\", end=\"\")\n",
    "        \"\"\"\n",
    "        if(rd_lr < flip_lr):\n",
    "            input = input[:, ::-1, :]\n",
    "            target = target[:, ::-1]\n",
    "        if(rd_ud < flip_ud):\n",
    "            input = input[::-1, :, :]\n",
    "            target = target[::-1, :, :]\n",
    "        \"\"\"\n",
    "        return input, target\n",
    "\n",
    "\"\"\"\n",
    "transform = T.Compose(\n",
    "    [T.RandomHorizontalFlip(p=0.5),\n",
    "     T.RandomVerticalFlip(p=0.5),\n",
    "     T.RandomRotation(degrees=(-20, 20))]\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "dataset = CustomImageDataset(input_dir=input_path, target_dir=target_path, title=TITLE)\n",
    "# print(len(dataset))\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - 50, 50])\n",
    "# print(len(train_dataset))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "# print(train_dataloader)\n",
    "# print(test_dataloader)\n",
    "\n",
    "data_num = len(train_dataset)\n",
    "parameters = \"aug_bin\" + str(num_bin) \\\n",
    "    + \"_batch\" + str(batch_size) \\\n",
    "        + \"_normdata\" + str(data_num)  \\\n",
    "            + \"_lr\" + str(learning_rate).split('.')[1] \\\n",
    "                + \"_\" + activation \\\n",
    "                    + \"_\" + Loss \\\n",
    "                        + \"_epoch\"+ str(epochs) \\\n",
    "                            + \"_Norm\" + str(norm)\n",
    "save_model = \"./Dataset/pretrained/model_\" + parameters + \".pth\"\n",
    "\n",
    "wandb.init(\n",
    "    project = \"event-based_NLOS\",\n",
    "\n",
    "    name = parameters,\n",
    "\n",
    "    config={\n",
    "        \"train_data_num\": data_num,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"activation\": activation,\n",
    "        \"train_loss\": Loss,\n",
    "        \"epochs\": epochs,\n",
    "        \"num_bin\": num_bin,\n",
    "        \"batches\": batch_size,\n",
    "        \"Norm\": norm\n",
    "    }\n",
    ")\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, epoch_num, batch):\n",
    "    model.train()\n",
    "    # X: input, y: target\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        print('batch: ', batch)\n",
    "        X, y = X.to(device).to(torch.float), y.to(device).to(torch.float)\n",
    "        # print(y.shape)\n",
    "\n",
    "        # Compute prediction error\n",
    "        # print('---------------------\\n\\n\\n')\n",
    "        # print('X: ', X)\n",
    "        # print('y: ', y)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        #loss = torch.sum(loss)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    loss_num = loss.item()\n",
    "    # writer.add_scalar(\"Train_Loss\", loss_num, epoch_num)\n",
    "    wandb.log({\"train_loss\": loss_num})\n",
    "    print('last batch value: ', batch)\n",
    "    print('loss_num: ', loss_num)\n",
    "    train_loss.append(loss_num)\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn, epoch_num):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        i=0\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device).to(torch.float), y.to(device).to(torch.float)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            i += 1\n",
    "    \n",
    "    loss_num = loss.item()\n",
    "    #writer.add_scalar(\"Test_Loss\", loss_num, epoch_num)\n",
    "    wandb.log({\"test_loss\": loss_num})\n",
    "    test_loss.append(loss_num)\n",
    "\n",
    "def main():\n",
    "    start = datetime.now()\n",
    "\n",
    "    # define model\n",
    "    unet = UNet_2D().to(device)\n",
    "    \n",
    "    #train_loss_fn = lpips.LPIPS(net='vgg').cuda().forward\n",
    "    train_loss_fn = nn.MSELoss()\n",
    "    test_loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(unet.parameters(), lr=learning_rate)\n",
    "\n",
    "    for t in range(epochs):\n",
    "        print(f\"[Epoch {t+1}]\")\n",
    "        train(train_dataloader, unet, train_loss_fn, optimizer, t, batch_size)\n",
    "        test(test_dataloader, unet, test_loss_fn, t)\n",
    "        print(\"\\n-------------------------------\")\n",
    "    end = datetime.now()\n",
    "    print(\"Done!\")\n",
    "    torch.save(unet, save_model)\n",
    "    print(\"Saved PyTorch Model: \" + save_model)\n",
    "    print(\"Start: \" + start.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    print(\"End: \" + end.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "    #writer.close()\n",
    "    wandb.finish()\n",
    "    plt.plot(train_loss, label='train')\n",
    "    plt.plot(test_loss, label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "event_nlos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
